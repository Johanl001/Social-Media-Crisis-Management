{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjfvUlMONSsod/yw3uSawl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Johanl001/Social-Media-Crisis-Management/blob/main/Data_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pipUoUYL3dRF"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Data pipeline: Reddit scraping (no API keys), cleaning, and save.\n",
        "\n",
        "Outputs:\n",
        "- mental_health_posts_with_classification.csv\n",
        "- mental_health_posts_with_classification.json\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import datetime\n",
        "from typing import List, Dict, Any\n",
        "from urllib.parse import quote as url_quote\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "from cleantext import clean\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72c8cdc8",
        "outputId": "1581fb95-a5f9-4b4e-c7d6-d13f2736ed39"
      },
      "source": [
        "%pip install cleantext"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cleantext\n",
            "  Downloading cleantext-1.1.4-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from cleantext) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->cleantext) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->cleantext) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->cleantext) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->cleantext) (4.67.1)\n",
            "Downloading cleantext-1.1.4-py3-none-any.whl (4.9 kB)\n",
            "Installing collected packages: cleantext\n",
            "Successfully installed cleantext-1.1.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------ Configuration ------------\n",
        "\n",
        "MENTAL_HEALTH_KEYWORDS: List[str] = [\n",
        "    \"depression\", \"depressed\", \"anxiety\", \"suicidal\",\n",
        "    \"suicide\", \"addiction\", \"substance abuse\", \"overwhelmed\",\n",
        "    \"hopeless\", \"self harm\", \"bipolar\", \"mental health\",\n",
        "    \"therapy\", \"crisis\", \"panic attack\"\n",
        "]\n",
        "\n",
        "OUTPUT_CSV = \"mental_health_posts_with_classification.csv\"\n",
        "OUTPUT_JSON = \"mental_health_posts_with_classification.json\"\n",
        "\n",
        "USER_AGENT = (\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "    \"Chrome/124.0 Safari/537.36\"\n",
        ")\n",
        "HTTP_TIMEOUT_SECS = 30\n",
        "DEFAULT_LIMIT_PER_KEYWORD = 100\n"
      ],
      "metadata": {
        "id": "pe6P6gL236mP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------ Utilities ------------\n",
        "\n",
        "def preprocess_text(text: str) -> str:\n",
        "    \"\"\"Clean text aggressively for downstream analysis.\"\"\"\n",
        "    if not isinstance(text, str) or text.strip() == \"\":\n",
        "        return \"\"\n",
        "    try:\n",
        "        cleaned_text = clean(\n",
        "            text,\n",
        "            extra_spaces=True,\n",
        "            lowercase=True,\n",
        "            numbers=True,\n",
        "            punct=True,\n",
        "            stopwords=True,\n",
        "            stp_lang=\"english\",\n",
        "            no_urls=True,\n",
        "            no_emails=True,\n",
        "            no_phone_numbers=True,\n",
        "            no_currency_symbols=True,\n",
        "            no_emoji=True,\n",
        "        )\n",
        "        # Collapse whitespace\n",
        "        cleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text).strip()\n",
        "        return cleaned_text\n",
        "    except Exception:\n",
        "        # Fallback minimal cleaning\n",
        "        text = re.sub(r\"http[s]?://\\S+\", \" \", text)\n",
        "        text = re.sub(r\"[^A-Za-z\\s]\", \" \", text)\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip().lower()\n",
        "        return text\n",
        "\n"
      ],
      "metadata": {
        "id": "-Ud-j2Px4HQc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------ Reddit Scraping (No API Keys) ------------\n",
        "\n",
        "def get_reddit_posts(keyword: str, limit: int = DEFAULT_LIMIT_PER_KEYWORD) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Scrape Reddit via public JSON (no keys) for a given keyword.\n",
        "    Filters results by MENTAL_HEALTH_KEYWORDS.\n",
        "    \"\"\"\n",
        "    results: List[Dict[str, Any]] = []\n",
        "    headers = {\"User-Agent\": USER_AGENT}\n",
        "    url = f\"https://www.reddit.com/search.json?q={url_quote(keyword)}&sort=new&limit={int(limit)}\"\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=HTTP_TIMEOUT_SECS)\n",
        "    except Exception:\n",
        "        return results\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        return results\n",
        "\n",
        "    try:\n",
        "        data = response.json()\n",
        "    except ValueError:\n",
        "        return results\n",
        "\n",
        "    for post in data.get(\"data\", {}).get(\"children\", []):\n",
        "        d = post.get(\"data\", {})\n",
        "        title = (d.get(\"title\") or \"\").strip()\n",
        "        selftext = (d.get(\"selftext\") or \"\").strip()\n",
        "        body = f\"{title}\\n{selftext}\".lower()\n",
        "\n",
        "        # Additional filter to ensure relevance\n",
        "        if not any(kw.lower() in body for kw in MENTAL_HEALTH_KEYWORDS):\n",
        "            continue\n",
        "\n",
        "        created_utc = d.get(\"created_utc\", 0)\n",
        "        timestamp = (\n",
        "            datetime.datetime.fromtimestamp(created_utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            if created_utc else \"\"\n",
        "        )\n",
        "\n",
        "        results.append({\n",
        "            \"platform\": \"reddit\",\n",
        "            \"post_id\": d.get(\"id\") or \"\",\n",
        "            \"timestamp\": timestamp,\n",
        "            \"author\": d.get(\"author\") or \"\",\n",
        "            \"title\": title,\n",
        "            \"content\": selftext,\n",
        "            \"subgroup\": d.get(\"subreddit\", \"reddit\"),\n",
        "            \"likes\": d.get(\"ups\", 0) or 0,\n",
        "            \"comments\": d.get(\"num_comments\", 0) or 0,\n",
        "            \"url\": f\"https://www.reddit.com{d.get('permalink','')}\",\n",
        "        })\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "Shkv7jGZ4Plo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------ Pipeline ------------\n",
        "\n",
        "def collect_posts_across_keywords(\n",
        "    keywords: List[str],\n",
        "    per_keyword_limit: int = DEFAULT_LIMIT_PER_KEYWORD,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Collect, deduplicate, clean, and return a DataFrame.\"\"\"\n",
        "    all_posts: List[Dict[str, Any]] = []\n",
        "\n",
        "    for kw in keywords:\n",
        "        print(f\"Collecting posts for keyword: {kw}\")\n",
        "        try:\n",
        "            all_posts.extend(get_reddit_posts(kw, limit=per_keyword_limit))\n",
        "        except Exception:\n",
        "            # Keep pipeline resilient; skip on scraping errors\n",
        "            continue\n",
        "\n",
        "    if not all_posts:\n",
        "        return pd.DataFrame(columns=[\n",
        "            \"platform\", \"post_id\", \"timestamp\", \"author\", \"title\", \"content\",\n",
        "            \"subgroup\", \"likes\", \"comments\", \"url\",\n",
        "            \"cleaned_title\", \"cleaned_content\",\n",
        "        ])\n",
        "\n",
        "    # Deduplicate by post_id primarily; fallback to URL if missin\n",
        "    df = pd.DataFrame(all_posts)\n",
        "    if \"post_id\" in df.columns:\n",
        "        df = df.drop_duplicates(subset=[\"post_id\"], keep=\"first\")\n",
        "    elif \"url\" in df.columns:\n",
        "        df = df.drop_duplicates(subset=[\"url\"], keep=\"first\")\n",
        "    else:\n",
        "        df = df.drop_duplicates(subset=[\"platform\", \"author\", \"timestamp\", \"title\"], keep=\"first\")\n",
        "\n",
        "    # Cleaning\n",
        "    df[\"cleaned_title\"] = df[\"title\"].map(preprocess_text)\n",
        "    df[\"cleaned_content\"] = df[\"content\"].map(preprocess_text)\n",
        "\n",
        "    # Sort newest first if timestamps are present\n",
        "    try:\n",
        "        df[\"timestamp_dt\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\")\n",
        "        df = df.sort_values(by=\"timestamp_dt\", ascending=False).drop(columns=[\"timestamp_dt\"])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def save_outputs(df: pd.DataFrame, csv_path: str = OUTPUT_CSV, json_path: str = OUTPUT_JSON) -> None:\n",
        "    df.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
        "    df.to_json(json_path, orient=\"records\", force_ascii=False)\n",
        "    print(f\"Saved CSV -> {os.path.abspath(csv_path)}\")\n",
        "    print(f\"Saved JSON -> {os.path.abspath(json_path)}\")\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    df = collect_posts_across_keywords(MENTAL_HEALTH_KEYWORDS, per_keyword_limit=DEFAULT_LIMIT_PER_KEYWORD)\n",
        "    print(f\"Collected rows: {len(df)}\")\n",
        "    save_outputs(df, OUTPUT_CSV, OUTPUT_JSON)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gox_GP6i4h_9",
        "outputId": "133235aa-2734-4bd2-96f9-db3bd2d0ac0a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting posts for keyword: depression\n",
            "Collecting posts for keyword: depressed\n",
            "Collecting posts for keyword: anxiety\n",
            "Collecting posts for keyword: suicidal\n",
            "Collecting posts for keyword: suicide\n",
            "Collecting posts for keyword: addiction\n",
            "Collecting posts for keyword: substance abuse\n",
            "Collecting posts for keyword: overwhelmed\n",
            "Collecting posts for keyword: hopeless\n",
            "Collecting posts for keyword: self harm\n",
            "Collecting posts for keyword: bipolar\n",
            "Collecting posts for keyword: mental health\n",
            "Collecting posts for keyword: therapy\n",
            "Collecting posts for keyword: crisis\n",
            "Collecting posts for keyword: panic attack\n",
            "Collected rows: 0\n",
            "Saved CSV -> /content/mental_health_posts_with_classification.csv\n",
            "Saved JSON -> /content/mental_health_posts_with_classification.json\n"
          ]
        }
      ]
    }
  ]
}